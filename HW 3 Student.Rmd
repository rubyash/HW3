---
title: "HW 3"
author: "Ruby Ashman"
date: "9/24/2024"
output: 
  html_document:
    number_sections: true
---

#

Let $E[X] = \mu$.  Show that $Var[X] := E[(X-E[X])^2] = E[X^2]-(E[X])^2$.  Note, all you have to do is show the second equality (the first is our definition from class). 
$E[(X-E[X])^2] = E[X^2]-(E[X])^2$
$E[(X-\mu)^2] = E[X^2]-(E[X])^2$
$E[X^2-2X\mu+\mu^2] = E[X^2]-(E[X])^2$
$E[X^2]-2E[X]\mu+\mu^2 = E[X^2]-(E[X])^2$ 
$E[X^2]-2\mu^2 +\mu^2 = E[X^2]-(E[X])^2$
$E[X^2]-\mu^2 = E[X^2]-(E[X])^2$
$E[X^2]-(E[X])^2 = E[X^2]-(E[X])^2$
# 

In the computational section of this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)

```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}
set.seed(1)
train=sample(1:nrow(dat), 100)
svmfit = svm(y ~ ., data = dat, kernel = "radial", gamma=1, cost = 1, scale = FALSE)
print(svmfit)
plot(svmfit, dat)

```

##

Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 

```{r}
train=sample(1:nrow(dat), 100)
svmfit = svm(y ~ ., data = dat, kernel = "radial", gamma=1, cost = 10000, scale = FALSE)
print(svmfit)
plot(svmfit, dat)
```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

While we are better capturing the training data, we are using the radial basis kernel, the most complex kernel, with a much higher cost value, which also increases the complexity of our model. Overfitting the data with an unnecessarily complex model is bad practice, and might cause issues when applying the model further down the line on a broader set of data. 

##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r}
#remove eval = FALSE in above
table(true=dat[-train,"y"], pred=predict(svmfit, newdata=dat[-train,]))
(76+24)/(76+24)
```
From the conclusion matrix, I calculated a classification error rate of 100%. Classifying the data points correctly 100% of the time is much greater than 80% benchmark often used, and indicates that the SVM algorithm in this case is correct 100% without disparity.

##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}
summary(dat[train,"y"])
26 / (74+26)

```

The proportion of class '2' in the training data was .26, and the proportion of class '2' in the data as a whole was .25. While it appears that the training data was broadly representative of the data, this small difference in proportion could be a contributing factor to any disparity, altough I believe it would be too strong to say there is an imbalance of the training and testing partitions, considering the proportion of class '2' in the testing data was .24. 
##

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  

```{r}

set.seed(1)
tune.out = tune(dat[train], cost= {0.1, 1, 10, 100, 1000}, gamma= {0.5,1,2,3,4})

```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  


```{r, eval = FALSE}
table(true=dat[-train,"y"], pred=predict(tune.out$best.model, newdata=dat[-train,]))
```

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

I was not able to get my tune.out variable to work without error, however I would think that this tuning variable might have balanced the training and testing partitions to produce a better fit for the data.

# 
Let's turn now to decision trees.  

```{r}

library(kmed)
data(heart)
library(tree)

```

## 

The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 

```{r}
names(heart)
hist(heart$age)
High = ifelse(heart$age<=55, "Low", "High")
heart = data.frame(heart, High)
High<-as.factor(High) 
heart<-data.frame(heart, High) 
heart <-heart[,-15]
str(heart)
```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
set.seed(101)
library(rpart.plot)
library(tree)

train=sample(1:nrow(heart), 240)
tree.heart <- rpart(High ~. -age, heart, subset = train) 
plot (tree.heart) 
text(tree.heart, pretty=0)
rpart.plot(tree.heart)

```


## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}
tree.pred = predict(heart.tree, heart[-train ,], type="class") 
with(heart[-train ,], table(tree.pred, High))
(17+22)/(17+4+14+22)
```

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}
set.seed(101)
cv.heart = cv.tree(tree.heart, FUN = prune.misclass)
cv.heart


prune.heart <- prune.misclass(tree.heart, best = 11)
plot(prune.heart)
text(prune.heart, pretty=0)

tree.pred = predict(prune.heart, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, High))
```


##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

Often, as accuracy decreases, interpretability increases, as making the tree less complex and easier to understand does decrease its accuracy. For example, fewer branches makes it easier to trace through the tree, but groups are classified less specifically, and more often misclassified.   

## 

Discuss the ways a decision tree could manifest algorithmic bias.  

A decision tree could manifest algorithmic bias by grouping together specific populations by a single factor, and not further dividing them for the purpose of conciseness/ interpretability. This could cause issues with generalizing/ predicting the outcomes/actions of these groups without recognizing they might behave differently from each other. Additionally, in a decision tree the splitting factors should be ordered from most to least important, and if the determined order of the priority of these factors was not appropriate groups could be separated in ways that are not truly most determinant of their characterstics, which could cause issues when applying the prediction data.